---
title: "Stepwise Regression with Negatively Correlated Covariates"
author : "Riley Ashton"
header-includes:
   - \usepackage{listings}
   - \usepackage{color}
output:
  pdf_document:
    highlight: pygments
    toc: true
    toc_depth: 2
    number_sections: true
  html_document:
    theme: spacelab
    df_print: paged
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(SelectionSimulator)
library(ggplot2); theme_set(theme_minimal())
library(dplyr)
source("./simulations/normal_test.R")
set.seed(7882)
```

\newpage

# Introduction

David Hamilton's 1987 [see @hamilton1987] paper showed that correlated variables are not always
redundant and that $R^2 > r^2_{yx_1} + r^2_{yx_2}$. In Section 3 of Hamilton's paper,
an extreme example was shown when two variables were negatively correlated
and neither were significant in explaining the response on their own,
but together they explained nearly all the variance ($R^2 \approx 1$).

Forward selection is a greedy algorithm that only examines one variable at a 
time. In cases of two highly negatively correlated variables, it is possible
that neither variable would be added, since it is possible that
neither variable would be significant on their own.

Algorithms that consider adding highly negatively correlated variables together,
and additionally any variables correlated with those are discussed and tested.

## Notation used

- p: number of covariates/predictors
- q: number of nonzero covariates/predictors (i.e. covariates in the model)
- n: number of observations
- [AIC: Akaike information criterion](https://en.wikipedia.org/wiki/Akaike_information_criterion)
- [BIC: Bayes information criterion](https://en.wikipedia.org/wiki/Bayesian_information_criterion)
- Step: name used for [traditional stepwise regression](https://en.wikipedia.org/wiki/Stepwise_regression) in this paper


\newpage

# Algorithm

## Step2

### Algorithm Idea
The first algorithm is called Step2. The purpose of Step2 improve upon the
traditional stepwise algorithm (in this report called Step), by examining
the correlation between covariates.

In the case of highly negatively correlated covariate pairs, Step2 will
consider the traditional choices of adding a single variable,
but also the option of adding both negatively correlated 
covariates to the model (refered to as a block). What constitutes a highly 
negatively block is determined by the parameter `cor_cutoff`. 
So a cutoff of -0.5 would mean
all covariate pairs with a correlation of less than -0.5 (e.g. -0.7) would
be additionally considered together. They would also be considered as singles,
as is every other variable not yet included in the model.

While both Step and Step2 are greedy algorithms, Step2 looks two steps
ahead at highly negatively correlated covariate pairs.

AIC or BIC is used as the metric for deciding which of the possible models
is the best at each step. This is controlled by the parameter `k`.
`k` = 2 means AIC is used and `k` = $log_e (n)$ means BIC by Step2


### Pseudocode

### Time and Space Complexity vs Step

The time complexity of the algorithm depends heavily on the choice of
`cor_cutoff`. The tradeoff is between a more accurate model and a longer
run time. The returns in model accuracy, to `cor_cutoff` closer to zero, decrease
rapidly. Step rarely has a problem with only slightly negative correlated variables,
so there is little for Step2 to improve on in those situations.

The time complexity of Step2 is generally within a constant of Step.
At worst it will be `p` times greater, where `p` is the number of parameters.
This is since Step chooses from at most `p` models to fit at each stage,
where Step2 choose from at most `p` choose 2 models.

The space complexity is largely unchanged from the traditional algorithm,
since only the AIC or BIC or each model is stored.


## Step3

### Algorithm Idea

Step3 does everything that Step2 does, but it also considers recursing on the
pairwise negatively correlated covariates, i.e. the blocks of Step2,
and considering anything highly
correlated with them and then anything highly correlated with those, etc until
it reaches a block size of `max_block_size`. 
Additionally, Step3 with a `max_block_size` of 2 is equivalent to Step2. 
The depth used in this paper is set at 3, so the algorithm will,
at most, consider including three covariates at a time.

What is classified as a highly correlated variable for the purposes of recursion
are set by the two parameters `recursive_cor_positive_cutoff` and 
`recursive_cor_negative_cutoff`. 

- Let $\text{RCN}$ be a short-form for `recursive_cor_negative_cutoff`
- Let $\text{RCP}$ be a short-form for `recursive_cor_positive_cutoff`
- Let $r(x,y)$ denote the sample correlation between $x$ and $y$
- Let $A$ denote the set of covariates
- Then $B$ denoting the set of pairs $(x,y)$ is defined as $B = \{(x, y) | x \in A \land y \in A \land x \neq y \land  r(x,y) <$ `cor_cutoff` $\}$
- Then $C$ denoting the set of triples $(x,y,z)$ is defined as
$$C = \{ (x,y,z) | (x,y) \in B \land z \in A \land (x \neq y \neq z) \land  \left(\text{block } x \, y \, z \right) \}$$
where $$\text{block } x \, y \, z = (r(x,z) < \text{RCN}) \lor (r(x,z) > \text{RCP}) \lor (r(y,z) < \text{RCN}) \lor (r(y,z) > \text{RCP})$$
- Likewise this pattern continues

### Pseudocode

### Time and Space Complexity vs Step

With a terrible choice of cutoff parameters (e.g. 0) and unlimited recursion depth,
Step3 will preform all subsets regression. All subsets regression
has an exponential time complexity in the number of covariates.

The worst case space and time complexity for a `max_block_size` of $\text{mbs}$ is
$\sim p ^ {(\text{mbs}-1)}$ times greater than Step.
With proper choices of cutoffs the time and space complexity is expected
to be within a constant of Step. 

Note that the expected proportional increase in running time could be computed
before running any simulations for any given data set and hyperparameters.
A possible future improvement could involve the algorithm guaranteeing
similar performance to Step by tuning the hyperparameters for a given data set.

\newpage

# Simulations

## Linear model

- $\mathbf Y = \beta_0 + \beta_1 \mathbf X_1 + \beta_2 \mathbf X_2 + \cdots + \epsilon$
- $\mathbf X_i$ are correlated, centred random normal variables
- Intercept ($\beta_0 = 9$)
- $\epsilon \sim \mathcal N(\mu = 0, \sigma = 1)$
- 1000 Simulations

## Generating Correlated Centred Random Normal Variables
The variables are generated according the the formula
$$\mathbf X =   \mathbf{C Z}$$
This generates a vector of correlated random normal variables 
$\mathbf X \sim \mathcal N(\boldsymbol \mu = 0, \mathbf \Sigma)$

where $\mathbf C$ is a $p \times p$ matrix that can be solved 
from the given covariate matrix, $\boldsymbol \Sigma$, such that

$$\boldsymbol \Sigma = \mathbf {C C^T} = 
\begin{bmatrix} 
\sigma_1^2 & \sigma_{12} & \cdots & \sigma_{1p} \\
\sigma_{21} & \sigma_2^2 & \cdots & \sigma_{2p} \\
\vdots & \vdots & \ddots & \vdots \\
\sigma_{1p} & \sigma_{2p} & \cdots &  \sigma_p^2
\end{bmatrix}$$

$\mathbf C$ is found using [Cholesky decomposition](https://en.wikipedia.org/wiki/Cholesky_decomposition)

Where $\mathbf Z$ is a vector of random normal variables
$$\mathbf Z = \begin{bmatrix} z_1 \\ z_2 \\ \vdots \\ z_p \end{bmatrix}$$
Where $Z_i \sim \mathcal N(\mu = 0, \sigma =  1)$


## Model 1 : Two negatively correlated
This model is designed to show the advantage of Step2 over Step
$$ \boldsymbol{\Sigma} =\begin{bmatrix} 
1   &-0.8 & 0 & 0 & 0 & 0 \\
-0.8& 1   & 0 & 0 & 0 & 0 \\
0   & 0   & 1 & 0 & 0 & 0 \\
0   & 0   & 0 & 1 & 0 & 0 \\
0   & 0   & 0 & 0 & 1 & 0 \\
0   & 0   & 0 & 0 & 0 & 1
\end{bmatrix} \hspace{10pt}
\hspace{20pt}
\boldsymbol{\beta} = \begin{bmatrix} 1 \\ 1 \\ 1 \\ 0 \\ 0 \\ 0 \end{bmatrix}$$

$n$ = 100

## Model 2: Three Negatively Correlated
This model is designed to show the advantage of Step3 over Step2 and Step

$$\boldsymbol{\Sigma} = \begin{bmatrix} 
1     & -0.8  & 0.25  & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
-0.8  & 1     & -0.75 & 0 & 0& 0 & 0 & 0 & 0 & 0 \\
0.25  & -0.75 & 1     & 0 & 0& 0 & 0 & 0 & 0 & 0 \\
0     & 0     & 0     & 1 & 0& 0 & 0 & 0 & 0 & 0 \\
0     & 0     & 0     & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\
0     & 0     & 0     & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\
0     & 0     & 0     & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\
0     & 0     & 0     & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\
0     & 0     & 0     & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\
0     & 0     & 0     & 0 & 0 & 0 & 0 & 0 & 0 & 1 \\
\end{bmatrix}
\hspace{10pt}
\hspace{20pt}
\boldsymbol{\beta}  = 
\begin{bmatrix} 1 \\ 1 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0
\end{bmatrix}$$
$n$ = 100


## Model 3 : Big p
This model is designed to show the advantages of Step2 and Step3 over Step
when $p >> n$. Note the covariates V6 through V100 are independent and not
included in the model.

$$ \boldsymbol \Sigma = \begin{bmatrix}
1 & -0.8  &  0.25 & 0     & 0     &  0 &\cdots  & 0\\
-0.8&   1   & -0.4  & 0     & 0     & 0 & \cdots & 0  \\
0.25& -0.4  &  1    & 0     & 0     & 0 & \cdots  & 0\\
0   &  0    &  0    & 1     & -0.75 & 0 & \cdots & 0\\
0   &  0    &  0    & -0.75 & 1     &  0 & \cdots & 0\\
0   & 0     & 0     & 0     & 0     &   1  &\cdots & 0\\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\
0   & 0     & 0     & 0     & 0     &    0 & \cdots & 1 
\end{bmatrix}
\hspace{10pt} \hspace{20pt}
\boldsymbol \beta = 
\begin{bmatrix} 1 \\ 1 \\ 1 \\ 1 \\ 1 \\ 0 \\ \vdots \\ 0 \\ \end{bmatrix}
$$
$n$ = 50


```{r}
# Model 1 : Two Negatively Correlated
cov_mat_twoNeg <- rbind(c( 1.00,  -0.8,  0.00, 0.00, 0.00, 0.00),
                        c( -0.8,  1.00,  0.00, 0.00, 0.00, 0.00),
                        c( 0.00,  0.00,  1.00, 0.00, 0.00, 0.00),
                        c( 0.00,  0.00,  0.00, 1.00, 0.00, 0.00),
                        c( 0.00,  0.00,  0.00, 0.00, 1.00, 0.00),
                        c( 0.00,  0.00,  0.00, 0.00, 0.00, 1.00))

coefficients_twoNeg <- c(V1 = 1, V2 = 1, V3 = 0, V4 = 0, V5 = 0, V6 = 0)

test_twoNeg <- normal_test(cov_mat_twoNeg, coefficients_twoNeg, intercept = 9, 
                        num_observations = 100, num_simulations = 1000,
                        file_name = "twoNeg.RDS")
```

```{r}
# Model 2 : Three Negatively Correlated
cov_mat_threeNeg <- Matrix::diag(replicate(10,1))
cov_mat_threeNeg[[1,2]] <- cov_mat_threeNeg[[2,1]] <- -0.8
cov_mat_threeNeg[[3,2]] <- cov_mat_threeNeg[[2,3]] <- -0.75
cov_mat_threeNeg[[3,1]] <- cov_mat_threeNeg[[1,3]] <- 0.25

threeNeg_coeff <- c(V1 = 1, V2 = 1, V3 = 1, V4 = 0, V5 =0,
                      V6 = 0, V7 =0, V8 = 0, V9 = 0, V10 = 0)

threeNeg <- normal_test(cov_mat_threeNeg, 
                        coefficients = threeNeg_coeff, intercept = 9, 
                        num_observations = 100, num_simulations = 1000,
                        file_name = "threeNeg.RDS")

# Model 3 : Big P
top_left <- rbind( c(1.00, -0.80,  0.25, 0.00, 0.00),
                    c(-0.80,  1.00, -0.4, 0.00, 0.00),
                    c( 0.25, -0.4,  1.00, 0.00, 0.00),
                    c( 0.00,  0.00,  0.00, 1.00, -0.75),
                    c( 0.00,  0.00,  0.00, -0.75, 1.00))

cov_mat <- as.matrix(Matrix::bdiag(top_left, diag(replicate(95,1))))

coefficients <- c(replicate(5,1), replicate(95, 0))
names(coefficients) <- sapply(1:100, function(x) paste0("V", x))

big_p <- normal_test(cov_mat, coefficients, intercept = 9, 
                     num_observations = 50, num_simulations = 100,
                     file_name = "big_p.RDS")
```

\newpage

# Results

## Model 1 : Two Negatively Correlated

In this case Step3 did not end up selecting any more covariates to add
to its block inclusion via its recursion, so it was identical to Step2. 
They were identical since only two
covariates were highly negatively correlated ($X_1$ and $X_2$) and
neither $X_1$ nor $X_2$ were highly correlated with any other covariates.
This meant that Step2 and Step3 considered including $X_1$ and $X_2$ in the
same step, but did not consider adding any other variables in the
same step.

Both Step2 and Step3 outperformed Step, since Step often ($\approx 30\%$) 
selected no covariates to include in the model (i.e. the intercept only model).
This is due to the highly negative sample correlation ($r \approx -0.8$)
between $X_1$ and $X_2$.

Since Step did not include $X_1$ and $X_2$ about 40% of the time,
while Step2 and Step3 included them nearly all the time,
the variance of the coefficient estimates for $X_1$ and $X_2$ were
much higher in the Step simulations than in Step2 or Step3.

```{r}
betas_heat_map(test_twoNeg)
test_mse_boxplot(test_twoNeg)
training_mse_boxplot(test_twoNeg)
```


```{r, results = "asis"}
inclusion_order(test_twoNeg)
coeff_bias(test_twoNeg)
coeff_variance(test_twoNeg)
```

```{r, results = "asis"}
#sample_cor_matrix(test_twoNeg)
```

$$\begin{bmatrix}
X    & -0.9/-0.63 & -0.29/0.29 & -0.35/0.29 & -0.29/0.29 & -0.34/0.31 \\
-0.9/-0.63 &      X    & -0.33/0.28 & -0.27/0.36 & -0.29/0.32 & -0.42/0.31 \\
-0.29/0.29 & -0.33/0.28 &      X    & -0.32/0.31 & -0.3/0.3 & -0.32/0.33 \\ 
-0.35/0.29 & -0.27/0.36 & -0.32/0.31 &      X    & -0.32/0.3 & -0.3/0.33 \\
-0.29/0.29 & -0.29/0.32 & -0.3/0.3 & -0.32/0.3 &      X    & -0.32/0.35 \\ 
-0.34/0.31 & -0.42/0.31 & -0.32/0.33 & -0.3/0.33 & -0.32/0.35 &      X
\end{bmatrix}$$

```{r}
proportion_included(test_twoNeg)
only_correct_predictors_included(test_twoNeg)
at_least_correct_predictors_included(test_twoNeg)
```

```{r}
test_mse_tables(test_twoNeg)
training_mse_tables(test_twoNeg)
```


## Model 2 : Three Negatively Correlated

```{r}
betas_heat_map(threeNeg)
inclusion_order(threeNeg)
test_mse_boxplot(threeNeg)
training_mse_boxplot(threeNeg)
test_mse_tables(threeNeg)
training_mse_tables(threeNeg)
coeff_bias(threeNeg)
coeff_variance(threeNeg)
```


## Model 3 : Big p

```{r}
betas_heat_map(big_p)
test_mse_boxplot(big_p)
training_mse_boxplot(big_p)
test_mse_tables(big_p)
training_mse_tables(big_p)
```

\newpage


# Appendix

## Future Inquiries
The hyperparameters of `cor_cutoff` and etc
in a general setting were not examined.
A general recommendation of values for these variables would be useful.


## Addition Plots

### Model 1

### Model 2

### Model 3


## Algorithm Source Code 

### Step 2
```{r}
if(knitr::is_html_output()) {
  print("View source code at https://github.com/riley-ashton/Selection/tree/master/R")
}
```
\lstinputlisting{../SelectionSimulator/R/Step2.R}

### Step 3
```{r}
if(knitr::is_html_output()) {
  print("View source code at https://github.com/riley-ashton/Selection/tree/master/R")
}
```
\lstinputlisting{../SelectionSimulator/R/Step3.R}
